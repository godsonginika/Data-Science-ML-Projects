{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "17a5b13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all libraries needed\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "056d7ccc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13580, 20)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the housing data\n",
    "X = pd.read_csv('melb_data.csv')\n",
    "y = X.Price\n",
    "\n",
    "# Drop the target column (Price)\n",
    "X.drop(['Price'], inplace=True, axis=1)\n",
    "\n",
    "# Check the size of the dataframe\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "785a1166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split my data\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Initialize my model\n",
    "model = XGBRegressor(n_estimators=100, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "493257d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical Columns: ['Rooms', 'Distance', 'Postcode', 'Bedroom2', 'Bathroom', 'Car', 'Landsize', 'BuildingArea', 'YearBuilt', 'Lattitude', 'Longtitude', 'Propertycount'], \n",
      "Categorical Columns: ['Type', 'Method', 'Regionname']\n"
     ]
    }
   ],
   "source": [
    "# PREPROCESSING\n",
    "# Check the datatype for all the columns\n",
    "X.dtypes\n",
    "\n",
    "# Seperate the numerical and object columns\n",
    "num_cols = [col for col in X_train.columns if X_train[col].dtype in ['int64', 'float64']]\n",
    "\n",
    "obj_cols = [col for col in X_train.columns if X_train[col].dtype in ['object'] \n",
    "            and X_train[col].nunique() < 10] # Make sure to check for high cardinality columns as well\n",
    "print(f'Numerical Columns: {num_cols}, \\nCategorical Columns: {obj_cols}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "6c9f185c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPROCESSING\n",
    "# Using column transformer, we impute our columns\n",
    "preprocessor = ColumnTransformer(transformers=[('num', SimpleImputer(strategy='mean'), num_cols),\n",
    "                                               ('obj', OneHotEncoder(handle_unknown='ignore'), obj_cols)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25efab10",
   "metadata": {},
   "source": [
    "##### This is the manual way to preprocess your data. The automatic way is using a pipeline. (Edit to view)\n",
    "<!-- # Fit and transform the data. This way we can avoid data leakage\n",
    "X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "X_valid_transformed = preprocessor.transform(X_valid)\n",
    "\n",
    "# This is to check our transformed data to see if there are any null values\n",
    "# It is not necessary at all\n",
    "print(X_train_transformed.shape) \n",
    "\n",
    "X_train_transformed_df = pd.DataFrame(\n",
    "    X_train_transformed,\n",
    "    columns=preprocessor.get_feature_names_out())\n",
    "\n",
    "# Check for missing values\n",
    "print(X_train_transformed_df.isnull().sum()) -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c9be4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error 154165.12371134022\n"
     ]
    }
   ],
   "source": [
    "# We create a pipeline for our model\n",
    "my_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                              ('model', model)])\n",
    "\n",
    "# Fit the model (Training the model)\n",
    "my_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model using validation data\n",
    "preds = my_pipeline.predict(X_valid)\n",
    "\n",
    "# Accuracy Metrics\n",
    "print('Mean Absolute Error', mean_absolute_error(preds, y_valid))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
